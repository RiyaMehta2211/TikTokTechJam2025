{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.1)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.8)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3d73a97a8b85a85b12dbdf703d079641f498c9f91ccc986279ea04ec0198656e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import chromadb\n",
    "import logging\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "import uuid\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocType(str, Enum):\n",
    "    SUMMARY = \"summary\"\n",
    "    PRECEDENT = \"precedent\"\n",
    "    DEFINITION = \"definition\"\n",
    "    LEGAL_TEXT = \"legal_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "logger = logging.getLogger(__name__)\n",
    "collection = chroma_client.get_or_create_collection(name=\"compliance_rules\")\n",
    "\n",
    "class ChunkModel(BaseModel):\n",
    "    # REMOVE THE __init__ METHOD COMPLETELY\n",
    "    # Just define the fields as class variables like this:\n",
    "\n",
    "    id: str = Field(\n",
    "        default_factory=lambda: f\"chunk_{uuid.uuid4().hex[:8]}\",\n",
    "        description=\"Auto-generated unique identifier for the chunk\"\n",
    "    )\n",
    "    content: str = Field(..., description=\"The main text content of the chunk\")\n",
    "    regulation: str = Field(..., description=\"Regulation this chunk pertains to\")\n",
    "    jurisdiction: str = Field(..., description=\"Geographic jurisdiction\")\n",
    "    doc_type: DocType = Field(..., description=\"Type of document\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"List of keywords for retrieval\")\n",
    "    source: Optional[str] = Field(None, description=\"Original source of the content\")\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"content\": self.content,\n",
    "            \"regulation\": self.regulation,\n",
    "            \"jurisdiction\": self.jurisdiction,\n",
    "            \"doc_type\": self.doc_type.value,\n",
    "            \"keywords\": self.keywords,\n",
    "            \"source\": self.source\n",
    "        }\n",
    "\n",
    "    def generateFullText(self) -> str:\n",
    "        lines = [\n",
    "            f\"REGULATION: {self.regulation}\",\n",
    "            f\"JURISDICTION: {self.jurisdiction}\",\n",
    "            f\"DOCUMENT TYPE: {self.doc_type.value}\",\n",
    "            f\"CONTENT: {self.content}\"\n",
    "        ]\n",
    "        if self.keywords:\n",
    "            keywords_str = \", \".join(self.keywords)\n",
    "            lines.append(f\"KEYWORDS: {keywords_str}\")\n",
    "        if self.source:\n",
    "            lines.append(f\"SOURCE: {self.source}\")\n",
    "        lines.append(f\"ID: {self.id}\")\n",
    "        return \". \".join(lines)\n",
    "    @classmethod\n",
    "    def from_json_dict(cls, json_data: dict) -> \"ChunkModel\":\n",
    "        # Use existing ID from JSON or generate new one\n",
    "        chunk_id = json_data.get('id', f\"chunk_{uuid.uuid4().hex[:8]}\")\n",
    "        return cls(\n",
    "            id=chunk_id,\n",
    "            content=json_data['content'],\n",
    "            regulation=json_data['regulation'],\n",
    "            jurisdiction=json_data['jurisdiction'],\n",
    "            doc_type=DocType(json_data['doc_type']),\n",
    "            keywords=json_data.get('keywords', []),\n",
    "            source=json_data.get('source')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "    def generate_embedding(self, chunk: ChunkModel):\n",
    "      return self.model.encode(chunk.generateFullText())\n",
    "    def generate_embedding_query(self, query: str):\n",
    "      return self.model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "class RegulationParser:\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def parse(self, file_path: str) -> List[ChunkModel]:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        documents = data.get(\"documents\", [])\n",
    "        chunks: List[ChunkModel] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            # Split content\n",
    "            for idx, content_chunk in enumerate(self.splitter.split_text(doc[\"content\"])):\n",
    "                chunk_data = {\n",
    "                    \"id\": f\"{doc['id']}_content_{idx}\",\n",
    "                    \"content\": content_chunk,\n",
    "                    \"regulation\": doc[\"regulation\"],\n",
    "                    \"jurisdiction\": doc[\"jurisdiction\"],\n",
    "                    \"doc_type\": doc[\"doc_type\"],\n",
    "                    \"keywords\": doc.get(\"trigger_keywords\", []),\n",
    "                    \"source\": doc[\"id\"]\n",
    "                }\n",
    "                chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
    "\n",
    "            # Split key_obligations\n",
    "            for i, obligation in enumerate(doc.get(\"key_obligations\", [])):\n",
    "                for j, chunk_text in enumerate(self.splitter.split_text(obligation)):\n",
    "                    chunk_data = {\n",
    "                        \"content\": chunk_text,\n",
    "                        \"regulation\": doc[\"regulation\"],\n",
    "                        \"jurisdiction\": doc[\"jurisdiction\"],\n",
    "                        \"doc_type\": doc[\"doc_type\"],\n",
    "                        \"keywords\": doc.get(\"trigger_keywords\", []),\n",
    "                        \"source\": doc[\"id\"]\n",
    "                    }\n",
    "                    chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
    "\n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkManager:\n",
    "    def __init__(self, db_path: str = \"./chroma_db\", collection_name: str = \"regulations\"):\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    def add_chunks(self, chunks: List[ChunkModel]):\n",
    "        self.collection.upsert(\n",
    "            documents=[c.content for c in chunks],\n",
    "            ids=[c.id for c in chunks],\n",
    "            embeddings=[EmbeddingGenerator().generate_embedding(c) for c in chunks],\n",
    "            metadatas=[{\n",
    "                \"regulation\": c.regulation,\n",
    "                \"jurisdiction\": c.jurisdiction,\n",
    "                \"doc_type\": str(c.doc_type),\n",
    "                \"keywords\": \", \".join(c.keywords) if c.keywords else \"\",\n",
    "                \"source\": c.source\n",
    "            } for c in chunks]\n",
    "        )\n",
    "\n",
    "    def query_chunks(self, query_embedding: List[float], k: int = 5):\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=k,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"]\n",
    "            )\n",
    "            if not results[\"metadatas\"] or not results[\"metadatas\"][0]:\n",
    "                return {\"results\": []}\n",
    "\n",
    "            num_results = len(results[\"metadatas\"][0])\n",
    "\n",
    "            return {\n",
    "                \"results\": [\n",
    "                    {\n",
    "                        \"metadata\": results[\"metadatas\"][0][i],\n",
    "                        \"document\": results[\"documents\"][0][i],\n",
    "                        \"distance\": results[\"distances\"][0][i],\n",
    "                        \"embedding\": results[\"embeddings\"][0][i].tolist() if results[\"embeddings\"] else None\n",
    "                    }\n",
    "                    for i in range(num_results)\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying chunks: {e}\")\n",
    "            return {\"results\": []}\n",
    "\n",
    "\n",
    "\n",
    "    def get_all_chunks(self) -> List[ChunkModel]:\n",
    "      \"\"\"Retrieve all chunks from the collection\"\"\"\n",
    "      results = self.collection.get(\n",
    "          include=[\"documents\", \"metadatas\", \"ids\", \"embeddings\"]\n",
    "      )\n",
    "      all_chunks = []\n",
    "\n",
    "      for doc, meta, cid, embedding in zip(results['documents'], results['metadatas'], results['ids'], results[\"embeddings\"]):\n",
    "          chunk_data = {\n",
    "              \"id\": cid,\n",
    "              \"content\": doc,\n",
    "              **meta,  # spread metadata fields directly\n",
    "              \"embedding\": embedding.tolist() if embedding is not None else None\n",
    "          }\n",
    "          all_chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
    "\n",
    "      return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "\n",
    "class ChunkComparator:\n",
    "\n",
    "    def __init__(self, chunk_manager: ChunkManager):\n",
    "        self.chunk_manager = chunk_manager\n",
    "        self.embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "    def query_by_text(self, query_text: str, top_k: int = 5, metadata_filter: Optional[dict] = None) -> List[ChunkModel]:\n",
    "        query_embedding = self.embedding_generator.model.encode(query_text)\n",
    "        return self.query_by_embedding(query_embedding, top_k=top_k, metadata_filter=metadata_filter)\n",
    "\n",
    "\n",
    "    def query_by_embedding(self, query_embedding: List[float], top_k: int = 5, metadata_filter: Optional[dict] = None) -> List[ChunkModel]:\n",
    "        all_chunks = self.chunk_manager.get_all_chunks()\n",
    "        if metadata_filter:\n",
    "            for key, value in metadata_filter.items():\n",
    "                all_chunks = [c for c in all_chunks if getattr(c, key) == value]\n",
    "\n",
    "        # Compute cosine similarity for each chunk\n",
    "        similarities = []\n",
    "        query_vec = np.array(query_embedding)\n",
    "        for chunk in all_chunks:\n",
    "            chunk_vec = np.array(chunk.embedding)  # assumes embedding stored in chunkModel\n",
    "            sim = cosine_similarity(query_vec, chunk_vec)\n",
    "            similarities.append((sim, chunk))\n",
    "\n",
    "        # Sort by similarity descending and return top_k\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_chunks = [chunk for _, chunk in similarities[:top_k]]\n",
    "        return top_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEngine:\n",
    "  def __init__(self, embedding_generator: EmbeddingGenerator, chromaVectorStore: ChunkManager, chunkGenerator: RegulationParser) -> None:\n",
    "    self.embedding_generator = EmbeddingGenerator()\n",
    "    self.chromaVectorStore = ChunkManager()\n",
    "    self.chunkGenerator = RegulationParser()\n",
    "\n",
    "  def initialize_database(self, filepath: str):\n",
    "    chunks = self.chunkGenerator.parse(filepath)\n",
    "    self.chromaVectorStore.add_chunks(chunks)\n",
    "    print(\"Chunks added to database...\")\n",
    "\n",
    "\n",
    "  def query_with_context(self, feature_description: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple RAG query that retrieves relevant compliance context and formats a prompt for LLM.\n",
    "    \"\"\"\n",
    "    query_embedding = self.embedding_generator.generate_embedding_query(feature_description)\n",
    "    query_result = self.chromaVectorStore.query_chunks(query_embedding, k=5)\n",
    "    relevant_chunks = query_result[\"results\"]\n",
    "    context = self._build_context(relevant_chunks)\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this feature description for geo-compliance requirements.\n",
    "    FEATURE DESCRIPTION:\n",
    "    {feature_description}\n",
    "    RELEVANT COMPLIANCE CONTEXT:\n",
    "    {context}\n",
    "    Answer these questions:\n",
    "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
    "    2. Why or why not? Provide clear reasoning based on the context.\n",
    "    3. Which specific regulations apply, if any?\n",
    "\n",
    "    Format your response as:\n",
    "    Requires Geo Logic: [Yes/No/Maybe]\n",
    "    Reasoning: [Your reasoning here]\n",
    "    Related Regulations: [Comma-separated list or None]\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "  def _build_context(self, chunks: list) -> str:\n",
    "    context_lines = []\n",
    "    #print(type(chunks))\n",
    "    for i, chunk in enumerate(chunks):\n",
    "      #print(chunk)\n",
    "      #print(f\"DEBUG: chunk {i} type = {type(chunk)}\")\n",
    "      #print(f\"DEBUG: chunk {i} value = {chunk}\")\n",
    "      context_lines.append(f\"--- Chunk {i+1} ---\")\n",
    "      context_lines.append(chunk['document'])\n",
    "      return \"\\n\".join(context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: trl 0.22.1 does not provide the extra 'sentencepiece'\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate trl[sentencepiece] huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = \"compliance_feature_analysis_with_desc.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df['input'] = df['feature_name'] + \". \" + df['feature_description']\n",
    "\n",
    "df['output'] = (\n",
    "    \"Flag: \" + df['flag'] + \"\\n\" +\n",
    "    \"Reasoning: \" + df['reasoning'] + \"\\n\")\n",
    "    # \"Related Regulations: \"+ df['related_regulations'] + \"\\n\")\n",
    "\n",
    "# Combine input + output into a single \"text\" column for SFTTrainer\n",
    "df['text'] = df['input'] + \"\\n\" + df['output']\n",
    "\n",
    "# Convert to HuggingFace Dataset and split train/test\n",
    "dataset = Dataset.from_pandas(df[['text']])\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f0b31282c74a0ca5b70092a878434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4925b9364ac94847ba92f09c838cb406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c054ab16ab784a59906f366513c8732d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d7c368ceb84ae79575ccf6538eb06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622963116a1346288a3ffb5b50e7e06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8239eb11cdc4a80a8419e004721ad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5992c3f4f344877a1a5d2c146035dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA 3.2 1B Instruct\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=True,\n",
    "    device_map=\"auto\",  # or \"cpu\" if you don't have GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc24650c00d415ab39731478b9e27bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e62fda04c61418692f63d940155409d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb09fb25a542430b86c878608e0eb35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2f7c81f656453cb366e9e61da21606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd27e4aa7344e17a8715e8929838535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762e14687633423fae0aeb1688af7362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.772000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=4.6529277801513675, metrics={'train_runtime': 9.5405, 'train_samples_per_second': 6.289, 'train_steps_per_second': 1.572, 'total_flos': 23846781874176.0, 'train_loss': 4.6529277801513675, 'entropy': 4.409536123275757, 'num_tokens': 4077.0, 'mean_token_accuracy': 0.2541874282062054, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Common attention layers\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Prepare Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-geo-ft\",\n",
    "    per_device_train_batch_size=1,      # adjust based on Colab GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=lora_config,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Start Training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG System Components...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3650f76396e4964bea80bbbbfe79d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3bf6819a1d4208b69178e46a9abb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5897025aa04a47679673428ddd99b683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb388fb231cf429bb2178f6cb5400a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683d365301844a16832f41768ba7b577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ae0306453a40549e0c8e14a6d55ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eb6da8b46f4a648b7a80a5e3789191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9523f60a0f95472d90c908f982a41f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98262fae2a04d1087bdda4e030792cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae890fd5aab14cc6b550d3422df8ead1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3204afd88d4fc4bc0bde363aa067e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading compliance knowledge base...\n",
      "Chunks added to database...\n",
      "Database initialized successfully!\n",
      "\n",
      "==================================================\n",
      "RUNNING COMPLIANCE CHECKS...\n",
      "==================================================\n",
      "\n",
      "Enter a feature description: exit\n",
      "Exiting system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the Compliance RAG System.\"\"\"\n",
    "\n",
    "    # Initialize components\n",
    "    print(\"Initializing RAG System Components...\")\n",
    "\n",
    "    embedding_generator = EmbeddingGenerator()\n",
    "    chroma_vector_store = ChunkManager()\n",
    "    regulation_parser = RegulationParser()\n",
    "\n",
    "    # Create RAG engine\n",
    "    rag_engine = RAGEngine(\n",
    "        embedding_generator=embedding_generator,\n",
    "        chromaVectorStore=chroma_vector_store,\n",
    "        chunkGenerator=regulation_parser\n",
    "    )\n",
    "\n",
    "    # Initialize database with compliance knowledge\n",
    "    print(\"Loading compliance knowledge base...\")\n",
    "    rag_engine.initialize_database(\"./sample_data/compliance_knowledge_base.json\")\n",
    "    print(\"Database initialized successfully!\")\n",
    "\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"We need to add a one-click report button for illegal videos\",\n",
    "        \"Add autoplay feature to video feed for all users\",\n",
    "        \"Create age verification system for new user signups\",\n",
    "        \"Implement content download blocking feature\",\n",
    "        \"Add parental controls for video viewing\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING COMPLIANCE CHECKS...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        query = input(\"\\nEnter a feature description: \").strip()\n",
    "        if query.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Exiting system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nChecking compliance for: '{query}'\")\n",
    "\n",
    "        # Generate the prompt with context\n",
    "        prompt = rag_engine.query_with_context(query)\n",
    "\n",
    "        # Prepare input for LLaMA\n",
    "        llama_input = f\"{prompt}\\nAnswer:\"\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(llama_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        # Only decode newly generated tokens (exclude the prompt echo)\n",
    "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        cleaned_lines = []\n",
    "        for line in answer.splitlines():\n",
    "            stripped = line.strip()\n",
    "            if stripped:  # keep non-empty lines only\n",
    "                cleaned_lines.append(stripped)\n",
    "\n",
    "        answer = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "        print(\"\\n=== LLaMA Output ===\")\n",
    "        print(answer.strip())\n",
    "\n",
    "\n",
    "    # print(\"\\n\" + \"=\"*50)\n",
    "    # print(\"SYSTEM READY FOR PRODUCTION USE!\")\n",
    "    # print(\"=\"*50)\n",
    "\n",
    "    # # Interactive mode example\n",
    "    # print(\"\\nTo use interactively, you would:\")\n",
    "    # print(\"1. Call rag_engine.query_with_context('your feature description')\")\n",
    "    # print(\"2. Send the resulting prompt to your LLM (GPT-4, Claude, etc.)\")\n",
    "    # print(\"3. Parse the LLM response for compliance decisions\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
