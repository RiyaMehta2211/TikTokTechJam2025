{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "if9jIEmXgc57",
        "outputId": "6fd8a49d-3e2e-4043-b437-c48d102f8260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.8)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtX2hI6wqRlH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import chromadb\n",
        "import logging\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "import uuid\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIC8v9Wt140w"
      },
      "outputs": [],
      "source": [
        "class DocType(str, Enum):\n",
        "    SUMMARY = \"summary\"\n",
        "    PRECEDENT = \"precedent\"\n",
        "    DEFINITION = \"definition\"\n",
        "    LEGAL_TEXT = \"legal_text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4e2V5ozCX7t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Delete old DB if exists\n",
        "if os.path.exists('./chroma_db'):\n",
        "    shutil.rmtree('./chroma_db')\n",
        "    print(f\"Deleted old Chroma DB at ./chroma_db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xklwe7hC7Hl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ceq-m4ZruFUC"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "logger = logging.getLogger(__name__)\n",
        "collection = chroma_client.get_or_create_collection(name=\"compliance_rules\")\n",
        "\n",
        "class ChunkModel(BaseModel):\n",
        "    # REMOVE THE __init__ METHOD COMPLETELY\n",
        "    # Just define the fields as class variables like this:\n",
        "\n",
        "    id: str = Field(\n",
        "        default_factory=lambda: f\"chunk_{uuid.uuid4().hex[:8]}\",\n",
        "        description=\"Auto-generated unique identifier for the chunk\"\n",
        "    )\n",
        "    content: str = Field(..., description=\"The main text content of the chunk\")\n",
        "    regulation: str = Field(..., description=\"Regulation this chunk pertains to\")\n",
        "    jurisdiction: str = Field(..., description=\"Geographic jurisdiction\")\n",
        "    doc_type: DocType = Field(..., description=\"Type of document\")\n",
        "    keywords: List[str] = Field(default_factory=list, description=\"List of keywords for retrieval\")\n",
        "    source: Optional[str] = Field(None, description=\"Original source of the content\")\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"id\": self.id,\n",
        "            \"content\": self.content,\n",
        "            \"regulation\": self.regulation,\n",
        "            \"jurisdiction\": self.jurisdiction,\n",
        "            \"doc_type\": self.doc_type.value,\n",
        "            \"keywords\": self.keywords,\n",
        "            \"source\": self.source\n",
        "        }\n",
        "\n",
        "    def generateFullText(self) -> str:\n",
        "        lines = [\n",
        "            f\"REGULATION: {self.regulation}\",\n",
        "            f\"JURISDICTION: {self.jurisdiction}\",\n",
        "            f\"DOCUMENT TYPE: {self.doc_type.value}\",\n",
        "            f\"CONTENT: {self.content}\"\n",
        "        ]\n",
        "        if self.keywords:\n",
        "            keywords_str = \", \".join(self.keywords)\n",
        "            lines.append(f\"KEYWORDS: {keywords_str}\")\n",
        "        if self.source:\n",
        "            lines.append(f\"SOURCE: {self.source}\")\n",
        "        lines.append(f\"ID: {self.id}\")\n",
        "        return \". \".join(lines)\n",
        "    @classmethod\n",
        "    def from_json_dict(cls, json_data: dict) -> \"ChunkModel\":\n",
        "        # Use existing ID from JSON or generate new one\n",
        "        chunk_id = json_data.get('id', f\"chunk_{uuid.uuid4().hex[:8]}\")\n",
        "        return cls(\n",
        "            id=chunk_id,\n",
        "            content=json_data['content'],\n",
        "            regulation=json_data['regulation'],\n",
        "            jurisdiction=json_data['jurisdiction'],\n",
        "            doc_type=DocType(json_data['doc_type']),\n",
        "            keywords=json_data.get('keywords', []),\n",
        "            source=json_data.get('source')\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY1BDuFD3v5y"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from typing import List\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model_name = model_name\n",
        "    def generate_embedding(self, chunk: ChunkModel):\n",
        "      return self.model.encode(chunk.generateFullText())\n",
        "    def generate_embedding_query(self, query: str):\n",
        "      return self.model.encode(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEthp0y7tIuo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "class RegulationParser:\n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "\n",
        "    def parse(self, file_path: str) -> List[ChunkModel]:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        documents = data.get(\"documents\", [])\n",
        "        chunks: List[ChunkModel] = []\n",
        "\n",
        "        for doc in documents:\n",
        "            # Split content\n",
        "            for idx, content_chunk in enumerate(self.splitter.split_text(doc[\"content\"])):\n",
        "                chunk_data = {\n",
        "                    \"id\": f\"{doc['id']}_content_{idx}\",\n",
        "                    \"content\": content_chunk,\n",
        "                    \"regulation\": doc[\"regulation\"],\n",
        "                    \"jurisdiction\": doc[\"jurisdiction\"],\n",
        "                    \"doc_type\": doc[\"doc_type\"],\n",
        "                    \"keywords\": doc.get(\"trigger_keywords\", []),\n",
        "                    \"source\": doc[\"id\"]\n",
        "                }\n",
        "                chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
        "\n",
        "            # Split key_obligations\n",
        "            for i, obligation in enumerate(doc.get(\"key_obligations\", [])):\n",
        "                for j, chunk_text in enumerate(self.splitter.split_text(obligation)):\n",
        "                    chunk_data = {\n",
        "                        \"content\": chunk_text,\n",
        "                        \"regulation\": doc[\"regulation\"],\n",
        "                        \"jurisdiction\": doc[\"jurisdiction\"],\n",
        "                        \"doc_type\": doc[\"doc_type\"],\n",
        "                        \"keywords\": doc.get(\"trigger_keywords\", []),\n",
        "                        \"source\": doc[\"id\"]\n",
        "                    }\n",
        "                    chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
        "\n",
        "        return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV4zGUXQ5niy"
      },
      "outputs": [],
      "source": [
        "class ChunkManager:\n",
        "    def __init__(self, db_path: str = \"./chroma_db\", collection_name: str = \"regulations\"):\n",
        "        self.client = chromadb.PersistentClient(path=db_path)\n",
        "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "    def add_chunks(self, chunks: List[ChunkModel]):\n",
        "        self.collection.upsert(\n",
        "            documents=[c.content for c in chunks],\n",
        "            ids=[c.id for c in chunks],\n",
        "            embeddings=[EmbeddingGenerator().generate_embedding(c) for c in chunks],\n",
        "            metadatas=[{\n",
        "                \"regulation\": c.regulation,\n",
        "                \"jurisdiction\": c.jurisdiction,\n",
        "                \"doc_type\": str(c.doc_type),\n",
        "                \"keywords\": \", \".join(c.keywords) if c.keywords else \"\",\n",
        "                \"source\": c.source\n",
        "            } for c in chunks]\n",
        "        )\n",
        "\n",
        "    def query_chunks(self, query_embedding: List[float], k: int = 5):\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=k,\n",
        "                include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"]\n",
        "            )\n",
        "            if not results[\"metadatas\"] or not results[\"metadatas\"][0]:\n",
        "                return {\"results\": []}\n",
        "\n",
        "            num_results = len(results[\"metadatas\"][0])\n",
        "\n",
        "            return {\n",
        "                \"results\": [\n",
        "                    {\n",
        "                        \"metadata\": results[\"metadatas\"][0][i],\n",
        "                        \"document\": results[\"documents\"][0][i],\n",
        "                        \"distance\": results[\"distances\"][0][i],\n",
        "                        \"embedding\": results[\"embeddings\"][0][i].tolist() if results[\"embeddings\"] else None\n",
        "                    }\n",
        "                    for i in range(num_results)\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying chunks: {e}\")\n",
        "            return {\"results\": []}\n",
        "\n",
        "\n",
        "\n",
        "    def get_all_chunks(self) -> List[ChunkModel]:\n",
        "      \"\"\"Retrieve all chunks from the collection\"\"\"\n",
        "      results = self.collection.get(\n",
        "          include=[\"documents\", \"metadatas\", \"ids\", \"embeddings\"]\n",
        "      )\n",
        "      all_chunks = []\n",
        "\n",
        "      for doc, meta, cid, embedding in zip(results['documents'], results['metadatas'], results['ids'], results[\"embeddings\"]):\n",
        "          chunk_data = {\n",
        "              \"id\": cid,\n",
        "              \"content\": doc,\n",
        "              **meta,  # spread metadata fields directly\n",
        "              \"embedding\": embedding.tolist() if embedding is not None else None\n",
        "          }\n",
        "          all_chunks.append(ChunkModel.from_json_dict(chunk_data))\n",
        "\n",
        "      return all_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSUxs19qHBD2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jylRLzYFckY"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "\n",
        "class ChunkComparator:\n",
        "\n",
        "    def __init__(self, chunk_manager: ChunkManager):\n",
        "        self.chunk_manager = chunk_manager\n",
        "        self.embedding_generator = EmbeddingGenerator()\n",
        "\n",
        "    def query_by_text(self, query_text: str, top_k: int = 5, metadata_filter: Optional[dict] = None) -> List[ChunkModel]:\n",
        "        query_embedding = self.embedding_generator.model.encode(query_text)\n",
        "        return self.query_by_embedding(query_embedding, top_k=top_k, metadata_filter=metadata_filter)\n",
        "\n",
        "\n",
        "    def query_by_embedding(self, query_embedding: List[float], top_k: int = 5, metadata_filter: Optional[dict] = None) -> List[ChunkModel]:\n",
        "        all_chunks = self.chunk_manager.get_all_chunks()\n",
        "        if metadata_filter:\n",
        "            for key, value in metadata_filter.items():\n",
        "                all_chunks = [c for c in all_chunks if getattr(c, key) == value]\n",
        "\n",
        "        # Compute cosine similarity for each chunk\n",
        "        similarities = []\n",
        "        query_vec = np.array(query_embedding)\n",
        "        for chunk in all_chunks:\n",
        "            chunk_vec = np.array(chunk.embedding)  # assumes embedding stored in chunkModel\n",
        "            sim = cosine_similarity(query_vec, chunk_vec)\n",
        "            similarities.append((sim, chunk))\n",
        "\n",
        "        # Sort by similarity descending and return top_k\n",
        "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_chunks = [chunk for _, chunk in similarities[:top_k]]\n",
        "        return top_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDlh37-JOW_9"
      },
      "outputs": [],
      "source": [
        "class RAGEngine:\n",
        "  def __init__(self, embedding_generator: EmbeddingGenerator, chromaVectorStore: ChunkManager, chunkGenerator: RegulationParser) -> None:\n",
        "    self.embedding_generator = EmbeddingGenerator()\n",
        "    self.chromaVectorStore = ChunkManager()\n",
        "    self.chunkGenerator = RegulationParser()\n",
        "\n",
        "  def initialize_database(self, filepath: str):\n",
        "    chunks = self.chunkGenerator.parse(filepath)\n",
        "    self.chromaVectorStore.add_chunks(chunks)\n",
        "    print(\"Chunks added to database...\")\n",
        "\n",
        "\n",
        "  def query_with_context(self, feature_description: str) -> str:\n",
        "    \"\"\"\n",
        "    Simple RAG query that retrieves relevant compliance context and formats a prompt for LLM.\n",
        "    \"\"\"\n",
        "    query_embedding = self.embedding_generator.generate_embedding_query(feature_description)\n",
        "    query_result = self.chromaVectorStore.query_chunks(query_embedding, k=5)\n",
        "    relevant_chunks = query_result[\"results\"]\n",
        "    context = self._build_context(relevant_chunks)\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this feature description for geo-compliance requirements.\n",
        "    FEATURE DESCRIPTION:\n",
        "    {feature_description}\n",
        "    RELEVANT COMPLIANCE CONTEXT:\n",
        "    {context}\n",
        "    Answer these questions:\n",
        "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
        "    2. Why or why not? Provide clear reasoning based on the context.\n",
        "    3. Which specific regulations apply, if any?\n",
        "\n",
        "    Format your response as:\n",
        "    Requires Geo Logic: [Yes/No/Maybe]\n",
        "    Reasoning: [Your reasoning here]\n",
        "    Related Regulations: [Comma-separated list or None]\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "  def _build_context(self, chunks: list) -> str:\n",
        "    context_lines = []\n",
        "    #print(type(chunks))\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      #print(chunk)\n",
        "      #print(f\"DEBUG: chunk {i} type = {type(chunk)}\")\n",
        "      #print(f\"DEBUG: chunk {i} value = {chunk}\")\n",
        "      context_lines.append(f\"--- Chunk {i+1} ---\")\n",
        "      context_lines.append(chunk['document'])\n",
        "      return \"\\n\".join(context_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAEixEUwzf8w",
        "outputId": "12d8519b-0aa9-4820-e3aa-44d7c1eabe2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate sentencepiece huggingface_hub --quiet\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# --- 1. Hugging Face login ---\n",
        "\n",
        "login(\"Create a hugging Face account\")\n",
        "\n",
        "# --- 2. Load LLaMA 3.2 1B Instruct ---\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    use_auth_token=True,\n",
        "    device_map=\"auto\",  # or \"cpu\" if you don't have GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIUr-jBGT2NH",
        "outputId": "bef59a51-8df6-40e1-c592-bf1fed963923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG System Components...\n",
            "Loading compliance knowledge base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks added to database...\n",
            "Database initialized successfully!\n",
            "\n",
            "==================================================\n",
            "RUNNING COMPLIANCE CHECKS...\n",
            "==================================================\n",
            "\n",
            "1. Testing: 'We need to add a one-click report button for illegal videos'\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    We need to add a one-click report button for illegal videos\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: User content reporting flow. Requirement: The feature must include a one-click reporting mechanism for users to flag illegal content. The feature must also tag reports coming from trusted flagger organizations and prioritize them for review. Technical Implementation: A dedicated backend service 'eu-content-moderation' handles all reports from EU member states. Logic exists to validate the user's geo-location and apply the correct UI strings and processing rules.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "   Generated prompt length: 1080 characters\n",
            "   Context retrieved: 1 relevant chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLaMA Output ===\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    We need to add a one-click report button for illegal videos\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: User content reporting flow. Requirement: The feature must include a one-click reporting mechanism for users to flag illegal content. The feature must also tag reports coming from trusted flagger organizations and prioritize them for review. Technical Implementation: A dedicated backend service 'eu-content-moderation' handles all reports from EU member states. Logic exists to validate the user's geo-location and apply the correct UI strings and processing rules.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "Answer: Requires Geo Logic: Yes\n",
            "\n",
            "    Answer: \n",
            "    Requires Geo Logic: Yes\n",
            "    Reasoning: The feature requires geo-specific compliance logic because the EU content moderation service is a dedicated backend service for EU member states. The feature must validate the user's geo-location to apply the correct UI strings and processing rules, which implies that the location is specific to the EU and not applicable to other regions. This is a geo-specific requirement to ensure that the content is not being flagged or reviewed for non-EU member states.\n",
            "\n",
            "2. Testing: 'Add autoplay feature to video feed for all users'\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Add autoplay feature to video feed for all users\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: Autoplay for video feed. Requirement: The autoplay feature must be disabled by default for any user identified as a minor or located in California. An age gate or parental consent check must be presented to enable it. Technical Implementation: The client-side app checks a user-age-preference service. If the user's age is under 18 or not verified, and their IP is in California, autoplay is disabled at the firmware level.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "   Generated prompt length: 1026 characters\n",
            "   Context retrieved: 1 relevant chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLaMA Output ===\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Add autoplay feature to video feed for all users\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: Autoplay for video feed. Requirement: The autoplay feature must be disabled by default for any user identified as a minor or located in California. An age gate or parental consent check must be presented to enable it. Technical Implementation: The client-side app checks a user-age-preference service. If the user's age is under 18 or not verified, and their IP is in California, autoplay is disabled at the firmware level.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "Answer: \n",
            "    Requires Geo Logic: Yes\n",
            "    Reasoning: The feature requires geo-specific compliance logic because the autoplay feature must be disabled by default for any user identified as a minor or located in California. This implies that the user's location and age must be verified before the autoplay feature is enabled. This is a geo-specific requirement to ensure compliance with California's age-based restrictions on access to adult content. The feature's technical implementation involves checking a user-age-preference service, which requires an IP address check for users in California. Therefore, this feature does not meet the requirements of geo-compliant services that require location-based checks and age verification. The client-side app checks a user-age-preference service, which must be implemented to ensure compliance with California's regulations.\n",
            "    Related Regulations: California Civil Code, California Penal Code, California Health and Safety Code, California Online Services Act, California Consumer Privacy Act, California Children's Services Act, California Digital Media Act, California Information Security Act, California Internet Services Act\n",
            "\n",
            "3. Testing: 'Create age verification system for new user signups'\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Create age verification system for new user signups\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Age Verification: Platforms must use independent, third-party services to verify age.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "   Generated prompt length: 682 characters\n",
            "   Context retrieved: 1 relevant chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLaMA Output ===\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Create age verification system for new user signups\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Age Verification: Platforms must use independent, third-party services to verify age.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "Answer: \n",
            "    Requires Geo Logic: Yes\n",
            "    Reasoning: Platforms must use independent, third-party services to verify age. Geo-specific compliance logic is required to ensure age verification is accurate and reliable across different regions, and to prevent age fraud. Independent services help to avoid reliance on local data or practices that may not be aligned with global standards.\n",
            "    Related Regulations: Age Verification Regulation (2010/114/EC), Age Discrimination Directive (2000/78/EC), Directive on the protection of individuals with regard to the processing of personal data and the protection of their rights and freedoms as set out in Article 7 of the Treaty on the Functioning of the European Union, and the EU General Data Protection Regulation (GDPR) (2016/679)\n",
            "\n",
            "4. Testing: 'Implement content download blocking feature'\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Implement content download blocking feature\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: User content reporting flow. Requirement: The feature must include a one-click reporting mechanism for users to flag illegal content. The feature must also tag reports coming from trusted flagger organizations and prioritize them for review. Technical Implementation: A dedicated backend service 'eu-content-moderation' handles all reports from EU member states. Logic exists to validate the user's geo-location and apply the correct UI strings and processing rules.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "   Generated prompt length: 1064 characters\n",
            "   Context retrieved: 1 relevant chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LLaMA Output ===\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Implement content download blocking feature\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: User content reporting flow. Requirement: The feature must include a one-click reporting mechanism for users to flag illegal content. The feature must also tag reports coming from trusted flagger organizations and prioritize them for review. Technical Implementation: A dedicated backend service 'eu-content-moderation' handles all reports from EU member states. Logic exists to validate the user's geo-location and apply the correct UI strings and processing rules.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "Answer: \n",
            "\n",
            "    Requires Geo Logic: [Yes]\n",
            "    Reasoning: The feature requires geo-specific compliance logic because the EU member states are located in a different time zone and the backend service 'eu-content-moderation' needs to validate the user's geo-location before applying the correct UI strings and processing rules. This is a geo-specific requirement to ensure the feature is accessible and usable across different regions.\n",
            "\n",
            "5. Testing: 'Add parental controls for video viewing'\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Add parental controls for video viewing\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: Autoplay for video feed. Requirement: The autoplay feature must be disabled by default for any user identified as a minor or located in California. An age gate or parental consent check must be presented to enable it. Technical Implementation: The client-side app checks a user-age-preference service. If the user's age is under 18 or not verified, and their IP is in California, autoplay is disabled at the firmware level.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "   Generated prompt length: 1017 characters\n",
            "   Context retrieved: 1 relevant chunks\n",
            "=== LLaMA Output ===\n",
            "\n",
            "    Analyze this feature description for geo-compliance requirements.\n",
            "    FEATURE DESCRIPTION:\n",
            "    Add parental controls for video viewing\n",
            "    RELEVANT COMPLIANCE CONTEXT:\n",
            "    --- Chunk 1 ---\n",
            "Feature: Autoplay for video feed. Requirement: The autoplay feature must be disabled by default for any user identified as a minor or located in California. An age gate or parental consent check must be presented to enable it. Technical Implementation: The client-side app checks a user-age-preference service. If the user's age is under 18 or not verified, and their IP is in California, autoplay is disabled at the firmware level.\n",
            "    Answer these questions:\n",
            "    1. Does this feature require geo-specific compliance logic? (Yes/No/Maybe)\n",
            "    2. Why or why not? Provide clear reasoning based on the context.\n",
            "    3. Which specific regulations apply, if any?\n",
            "\n",
            "    Format your response as:\n",
            "    Requires Geo Logic: [Yes/No/Maybe]\n",
            "    Reasoning: [Your reasoning here]\n",
            "    Related Regulations: [Comma-separated list or None]\n",
            "    \n",
            "Answer: \n",
            "    Requires Geo Logic: [Yes]\n",
            "    Reasoning: This feature requires geo-specific compliance logic because it involves the disabling of autoplay for minors or users in California. The client-side app checks a user-age-preference service, which suggests that the system is aware of the user's age and location. This implies that the system needs to be aware of the user's location to apply the geo-specific compliance logic. Therefore, it is likely that the system needs to use geo-compliance logic to determine whether the autoplay feature should be enabled or disabled based on the user's location. \n",
            "\n",
            "    Related Regulations: None \n",
            "\n",
            "    Answer: \n",
            "    Requires Geo Logic: [Yes]\n",
            "    Reasoning: This feature requires geo-specific compliance logic because it involves the disabling of autoplay for minors or users in California. The client-side app checks a user-age-preference service, which suggests that the system is aware of the user's age and location. This implies that the system needs to be aware of the user's\n",
            "\n",
            "==================================================\n",
            "SYSTEM READY FOR PRODUCTION USE!\n",
            "==================================================\n",
            "\n",
            "To use interactively, you would:\n",
            "1. Call rag_engine.query_with_context('your feature description')\n",
            "2. Send the resulting prompt to your LLM (GPT-4, Claude, etc.)\n",
            "3. Parse the LLM response for compliance decisions\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Compliance RAG System.\"\"\"\n",
        "\n",
        "    # Initialize components\n",
        "    print(\"Initializing RAG System Components...\")\n",
        "\n",
        "    embedding_generator = EmbeddingGenerator()\n",
        "    chroma_vector_store = ChunkManager()\n",
        "    regulation_parser = RegulationParser()\n",
        "\n",
        "    # Create RAG engine\n",
        "    rag_engine = RAGEngine(\n",
        "        embedding_generator=embedding_generator,\n",
        "        chromaVectorStore=chroma_vector_store,\n",
        "        chunkGenerator=regulation_parser\n",
        "    )\n",
        "\n",
        "    # Initialize database with compliance knowledge\n",
        "    print(\"Loading compliance knowledge base...\")\n",
        "    rag_engine.initialize_database(\"./sample_data/compliance_knowledge_base.json\")\n",
        "    print(\"Database initialized successfully!\")\n",
        "\n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"We need to add a one-click report button for illegal videos\",\n",
        "        \"Add autoplay feature to video feed for all users\",\n",
        "        \"Create age verification system for new user signups\",\n",
        "        \"Implement content download blocking feature\",\n",
        "        \"Add parental controls for video viewing\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RUNNING COMPLIANCE CHECKS...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\n{i}. Testing: '{query}'\")\n",
        "\n",
        "        # Generate the prompt with context\n",
        "        prompt = rag_engine.query_with_context(query)\n",
        "\n",
        "        # In a real system, you'd send this to an LLM API\n",
        "        # For now, we'll just print the prompt structure\n",
        "        print(prompt)\n",
        "        print(f\"   Generated prompt length: {len(prompt)} characters\")\n",
        "        print(f\"   Context retrieved: {prompt.count('Chunk')} relevant chunks\")\n",
        "\n",
        "        # Simulate LLM response (you'll replace this with actual LLM call)\n",
        "        # print(\"    Would send to LLM for analysis\")\n",
        "\n",
        "        # Prepare input for LLaMA ---\n",
        "        llama_input = f\"{prompt}\\nAnswer:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(llama_input, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate output ---\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode output ---\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(\"=== LLaMA Output ===\")\n",
        "        print(answer)\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SYSTEM READY FOR PRODUCTION USE!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Interactive mode example\n",
        "    print(\"\\nTo use interactively, you would:\")\n",
        "    print(\"1. Call rag_engine.query_with_context('your feature description')\")\n",
        "    print(\"2. Send the resulting prompt to your LLM (GPT-4, Claude, etc.)\")\n",
        "    print(\"3. Parse the LLM response for compliance decisions\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsl6ylO54efo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}